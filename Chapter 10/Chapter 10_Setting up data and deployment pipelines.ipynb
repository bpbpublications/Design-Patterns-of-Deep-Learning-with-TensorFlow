{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56da92d",
   "metadata": {},
   "source": [
    "### Chapter 10 - Setting up data and deployment pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82617ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4b468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TFRecord file paths\n",
    "train_filename = '/content/data/fashion_mnist_train.tfrecord'\n",
    "test_filename = '/content/data/fashion_mnist_test.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af454915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to serialize the data and write to the TFRecord file\n",
    "def serialize_example(image, label):\n",
    "    image_raw = image.numpy().tobytes()\n",
    "\n",
    "    # Convert the byte string to a list of floats\n",
    "    image_float = np.frombuffer(image_raw, dtype=np.float32).tolist()\n",
    "\n",
    "    # Create the feature dictionary\n",
    "    feature = {\n",
    "    'image': tf.train.Feature(float_list=tf.train.FloatList(value=image_float)),\n",
    "    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label.numpy()])),\n",
    "    }\n",
    "    # Create an Example object\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "    return example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ed622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tfrecordWriter\n",
    "def tfrecordWriter(filename,images,labels):\n",
    "    # Write the training data to the TFRecord file\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for i in range(len(images)):\n",
    "            image = tf.convert_to_tensor(images[i])\n",
    "            label = tf.convert_to_tensor(labels[i])\n",
    "            example_proto = serialize_example(image, label)\n",
    "            writer.write(example_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5406ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the training examples\n",
    "tfrecordWriter(train_filename,train_images,train_labels)\n",
    "# Writing the test examples\n",
    "tfrecordWriter(test_filename,test_images,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f7e65",
   "metadata": {},
   "source": [
    "#### Tensorflow extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tfx\n",
    "!pip install tensorflow_model_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c278ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import tensorflow as tf\n",
    "import absl\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import ImportExampleGen\n",
    "from tfx.components import Pusher\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Transform\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tfx_root = '/content/tfx_root/'\n",
    "_data_root = '/content/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b77888",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext(pipeline_root=_tfx_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f642a60",
   "metadata": {},
   "source": [
    "##### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define example gen and run the context\n",
    "example_gen = ImportExampleGen(input_base=_data_root)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e16972f",
   "metadata": {},
   "source": [
    "##### Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes statistics over data for visualization and example validation.\n",
    "statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8989c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates schema based on statistics files.\n",
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e220ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs anomaly detection based on statistics and data schema.\n",
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea70ec",
   "metadata": {},
   "source": [
    "#### Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/mnist_transform.py\n",
    "\n",
    "import absl\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Defining the keys to access data and labels\n",
    "\n",
    "IMAGE_KEY = 'image'\n",
    "LABEL_KEY = 'label'\n",
    "\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "# TFX Transform will call this function.\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "\n",
    "    Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "\n",
    "    Returns:\n",
    "    Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "\n",
    "    # Transformation to the images\n",
    "    outputs[transformed_name(IMAGE_KEY)] = (tft.scale_by_min_max(inputs[IMAGE_KEY], -0.5, 0.5))\n",
    "    # Apply transformations to labels if any\n",
    "    outputs[transformed_name(LABEL_KEY)] = inputs[LABEL_KEY]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa305730",
   "metadata": {},
   "outputs": [],
   "source": [
    "_module_file = '/content/mnist_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb82354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs transformations and feature engineering in training and serving.\n",
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=_module_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b6dd2",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /content/mnist_train.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tfx.components.trainer.fn_args_utils import FnArgs\n",
    "from typing import List\n",
    "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
    "from tfx_bsl.tfxio import dataset_options\n",
    "import absl\n",
    "\n",
    "IMAGE_KEY = 'image'\n",
    "LABEL_KEY = 'label'\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example.\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return model(transformed_features)\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "# Function to generate features and labels for training\n",
    "def input_fn(file_pattern: List[str],\n",
    "             data_accessor: DataAccessor,\n",
    "             tf_transform_output: tft.TFTransformOutput,\n",
    "             batch_size: int = 128) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      dataset_options.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=transformed_name(LABEL_KEY)),\n",
    "      tf_transform_output.transformed_metadata.schema).repeat()\n",
    "\n",
    "# Model building function\n",
    "def build_keras_model() -> tf.keras.Model:\n",
    "    \"\"\"Creates a DNN Keras model for classifying MNIST data.\n",
    "\n",
    "    Returns:\n",
    "    A Keras Model.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "      tf.keras.layers.InputLayer(\n",
    "          input_shape=(196,), name=transformed_name(IMAGE_KEY)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\n",
    "      metrics=['sparse_categorical_accuracy'])\n",
    "    model.summary(print_fn=absl.logging.info)\n",
    "    return model\n",
    "\n",
    "# Main function for trainer. TFX Trainer will call this function.\n",
    "def run_fn(fn_args: FnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "    Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = input_fn(fn_args.train_files, fn_args.data_accessor,\n",
    "                                tf_transform_output, 40)\n",
    "    eval_dataset = input_fn(fn_args.eval_files, fn_args.data_accessor,\n",
    "                               tf_transform_output, 40)\n",
    "\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "    model = build_keras_model()\n",
    "\n",
    "    # Write logs to path\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=fn_args.model_run_dir, update_freq='epoch')\n",
    "\n",
    "    model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps,\n",
    "      callbacks=[tensorboard_callback])\n",
    "\n",
    "    signatures = {\n",
    "      'serving_default':\n",
    "          _get_serve_tf_examples_fn(\n",
    "              model, tf_transform_output).get_concrete_function(\n",
    "                  tf.TensorSpec(shape=[None], dtype=tf.string, name='examples'))\n",
    "    }\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3234cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the component ID and path to the train module file\n",
    "component_id = 'Trainer.mnist'\n",
    "train_module_file = '/content/mnist_train.py'\n",
    "# Initialise and run the trainer componenet\n",
    "trainer = Trainer(\n",
    "        module_file=train_module_file,\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=500),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=500)).with_id(component_id)\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673274ca",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accuracy threshold\n",
    "accuracy_threshold: float = 0.8\n",
    "# Define the evaluation configuration\n",
    "eval_config = tfma.EvalConfig(\n",
    "      model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "      slicing_specs=[tfma.SlicingSpec()],\n",
    "      metrics_specs=[\n",
    "          tfma.MetricsSpec(metrics=[\n",
    "              tfma.MetricConfig(\n",
    "                  class_name='SparseCategoricalAccuracy',\n",
    "                  threshold=tfma.MetricThreshold(\n",
    "                      value_threshold=tfma.GenericValueThreshold(\n",
    "                          lower_bound={'value': accuracy_threshold})))\n",
    "          ])\n",
    "      ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the evaluator\n",
    "evaluator = Evaluator(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      model=trainer.outputs['model'],\n",
    "      eval_config=eval_config).with_id('Evaluator.mnist')\n",
    "# Run the evaluator\n",
    "context.run(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e07cb",
   "metadata": {},
   "source": [
    "#### Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c717fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if its not created\n",
    "!mkdir /content/serving_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b856f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path for the model serving directory\n",
    "_serving_model_dir = '/content/serving_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4960aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the pusher\n",
    "pusher = Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      model_blessing=evaluator.outputs['blessing'],\n",
    "      push_destination=pusher_pb2.PushDestination(\n",
    "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "              base_directory=_serving_model_dir))).with_id('Pusher.mnist')\n",
    "# Run the pusher\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c428efb",
   "metadata": {},
   "source": [
    "#### Creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61949af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     train_module_file: str,\n",
    "                    transform_module_file: str,\n",
    "                    serving_model_dir: str,\n",
    "                     metadata_path: str,\n",
    "                     beam_pipeline_args: List[str],\n",
    "                     accuracy_threshold: float = 0.8) -> pipeline.Pipeline:\n",
    "\n",
    "\n",
    "    # Define example gen\n",
    "    example_gen = ImportExampleGen(input_base=data_root)\n",
    "    print(\"Completed example_gen\")\n",
    "    # Computes statistics over data\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "    print(\"Completed statistics_gen\")\n",
    "    # Generates schema based on statistics files.\n",
    "    schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
    "    print(\"Completed schema_gen\")\n",
    "    # Anomaly detection based on statistics and data schema.\n",
    "    example_validator = ExampleValidator(statistics=statistics_gen.outputs['statistics'],schema=schema_gen.outputs['schema'])\n",
    "    print(\"Completed example_validator\")\n",
    "    # Transformations and feature engineering\n",
    "    transform = Transform(examples=example_gen.outputs['examples'],schema=schema_gen.outputs['schema'],module_file=transform_module_file)\n",
    "    print(\"Completed transform\")\n",
    "    # Initialise the trainer componenet\n",
    "    component_id = 'Trainer.mnist'\n",
    "    trainer = Trainer(\n",
    "        module_file=train_module_file,\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=500),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=500)).with_id(component_id)\n",
    "    print(\"Completed trainer\")\n",
    "    # Quality validation of the candidate model.\n",
    "    eval_config = tfma.EvalConfig(\n",
    "      model_specs=[tfma.ModelSpec(label_key='label')],\n",
    "      slicing_specs=[tfma.SlicingSpec()],\n",
    "      metrics_specs=[\n",
    "          tfma.MetricsSpec(metrics=[\n",
    "              tfma.MetricConfig(\n",
    "                  class_name='SparseCategoricalAccuracy',\n",
    "                  threshold=tfma.MetricThreshold(\n",
    "                      value_threshold=tfma.GenericValueThreshold(\n",
    "                          lower_bound={'value': accuracy_threshold})))\n",
    "          ])\n",
    "      ])\n",
    "\n",
    "    # Compute the evaluation statistics over features of a model.\n",
    "    evaluator = Evaluator(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      model=trainer.outputs['model'],\n",
    "      eval_config=eval_config).with_id('Evaluator.mnist')\n",
    "    print(\"Completed evaluator\")\n",
    "\n",
    "\n",
    "    # Push the model to the destination folder\n",
    "    pusher = Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      model_blessing=evaluator.outputs['blessing'],\n",
    "      push_destination=pusher_pb2.PushDestination(\n",
    "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir))).with_id('Pusher.mnist')\n",
    "    print(\"Completed pusher\")\n",
    "    # Define the pipeline\n",
    "    pipeline_mnist = pipeline.Pipeline(pipeline_name=pipeline_name,pipeline_root=pipeline_root,\n",
    "      components=[\n",
    "          example_gen,\n",
    "          statistics_gen,\n",
    "          schema_gen,\n",
    "          example_validator,\n",
    "          transform,\n",
    "          trainer,\n",
    "          evaluator,\n",
    "          pusher,\n",
    "      ],\n",
    "      enable_cache=True,\n",
    "      metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "          metadata_path),\n",
    "      beam_pipeline_args=beam_pipeline_args)\n",
    "    print(\"Completed pipeline_mnist\")\n",
    "    # Return the pipeline\n",
    "    return pipeline_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir serving_model\n",
    "!mkdir pipelines\n",
    "!mkdir metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b22248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of pipeline\n",
    "pipeline_name = 'mnist_pipeline'\n",
    "# Path to tfx root\n",
    "tfx_root = '/content/tfx_root/'\n",
    "# Path to .tfrecords for the dataset\n",
    "data_root = '/content/data/'\n",
    "# Combined module file for transform and trainer\n",
    "train_module_file = '/content/mnist_train.py'\n",
    "transform_module_file = '/content/mnist_transform.py'\n",
    "\n",
    "# Path which can be listened to by the model server. Pusher will output the\n",
    "# trained model here.\n",
    "serving_model_dir = '/content/serving_model/'\n",
    "# Root path for pipelines\n",
    "pipeline_root = '/content/pipelines/mnist_pipeline'\n",
    "# Sqlite ML-metadata db path.\n",
    "metadata_path = '/content/metadata/metadata.db'\n",
    "# Pipeline arguments for Beam powered Components.\n",
    "beam_pipeline_args = [\n",
    "    '--direct_running_mode=multi_processing',\n",
    "    # 0 means auto-detect based on on the number of CPUs available\n",
    "    # during execution time.\n",
    "    '--direct_num_workers=0',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29789bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeamDagRunner for running the pipeline\n",
    "BeamDagRunner().run(\n",
    "      create_pipeline(\n",
    "          pipeline_name=pipeline_name,\n",
    "          pipeline_root=pipeline_root,\n",
    "          data_root=data_root,\n",
    "          train_module_file=train_module_file,\n",
    "          transform_module_file=transform_module_file,\n",
    "          serving_model_dir=serving_model_dir,\n",
    "          metadata_path=metadata_path,\n",
    "          beam_pipeline_args=beam_pipeline_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b3dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54529957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec84d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
