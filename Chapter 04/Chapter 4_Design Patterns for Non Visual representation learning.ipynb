{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8485e823",
   "metadata": {},
   "source": [
    "#### Chapter 4 - Design Patterns for Non Visual Representation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    'This is the first sentence.',\n",
    "    'And here is another sentence.',\n",
    "    'Yet another sentence follows.'\n",
    "]\n",
    "\n",
    "# Create tokenizer with character-level tokenization\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Tokenize the sentences at the character level\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(tokenized_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ad485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences\n",
    "sentences = [\n",
    "    'This is the first sentence.',\n",
    "    'And here is another sentence.',\n",
    "   'Yet another sentence follows.'\n",
    "]\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(tokenized_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2324b5",
   "metadata": {},
   "source": [
    "##### Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "import tempfile\n",
    "\n",
    "# Sample sentence\n",
    "sentence = 'This is the first sentence.'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "# Create a temporary file and write the sentence to it\n",
    "temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "temp_file.write(sentence.encode('utf-8'))\n",
    "temp_file.close()\n",
    "\n",
    "# Train the tokenizer on the sentence\n",
    "tokenizer.train([temp_file.name])\n",
    "\n",
    "# Encode the sentence into subword tokens\n",
    "encoding = tokenizer.encode(sentence)\n",
    "\n",
    "# Get the subword tokens\n",
    "subword_tokens = encoding.tokens\n",
    "\n",
    "print(subword_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4fd31",
   "metadata": {},
   "source": [
    "#### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34812258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vocabulary size and embedding dimension\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "# Create the embedding layer\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Example usage: Pass token indices to the embedding layer\n",
    "token_indices = [1, 3, 5, 7, 9]\n",
    "token_indices_tensor = tf.constant(token_indices)\n",
    "embedded_tokens = embedding_layer(token_indices_tensor)\n",
    "\n",
    "print(embedded_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da43529b",
   "metadata": {},
   "source": [
    "#### Sequence labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the input sequence as text\n",
    "input_sequence = [\"This is an example sentence.\", \"Another sentence for testing.\"]\n",
    "\n",
    "# Define the POS labels\n",
    "pos_labels = [['DET', 'VERB', 'DET', 'NOUN', 'NOUN', 'PUNCT'], ['DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT']]\n",
    "\n",
    "# Define the maximum length for padding\n",
    "maxlen = 10\n",
    "\n",
    "# Define the vocabulary size and embedding dimension\n",
    "vocab_size = 100\n",
    "embedding_dim = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834977d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input sequence\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(input_sequence)\n",
    "input_sequence = tokenizer.texts_to_sequences(input_sequence)\n",
    "\n",
    "# Convert the input sequence and labels to numpy arrays\n",
    "input_sequence = tf.keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=maxlen, padding='post')\n",
    "# Convert POS labels to numerical indices\n",
    "pos_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>', filters='')\n",
    "pos_tokenizer.fit_on_texts([label for sentence_labels in pos_labels for label in sentence_labels])\n",
    "pos_labels_num = [pos_tokenizer.texts_to_sequences(sentence_labels) for sentence_labels in pos_labels]\n",
    "\n",
    "# Pad the POS labels\n",
    "pos_labels_padded = tf.keras.preprocessing.sequence.pad_sequences(pos_labels_num, maxlen=maxlen, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN parameters\n",
    "rnn_units = 64\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.SimpleRNN(rnn_units, return_sequences=True),\n",
    "    tf.keras.layers.Dense(len(pos_tokenizer.word_index) + 1, activation='softmax')  # +1 for padding\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49630f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(input_sequence, pos_labels_padded, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09c13d",
   "metadata": {},
   "source": [
    "#### Sequence Classification Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab27860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define the input sequence as text\n",
    "input_sequence = [\"This is an example sentence.\", \"Another sentence for testing.\"]\n",
    "\n",
    "# Define the labels\n",
    "labels = [0, 1]\n",
    "\n",
    "# Tokenize the input sequence\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(input_sequence)\n",
    "input_sequence = tokenizer.texts_to_sequences(input_sequence)\n",
    "\n",
    "# Convert the input sequence and labels to numpy arrays\n",
    "input_sequence = tf.keras.preprocessing.sequence.pad_sequences(input_sequence, maxlen=maxlen, padding='post')\n",
    "# Convert the labels to numpy arrays\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Define the vocabulary size and embedding dimension\n",
    "vocab_size = 100\n",
    "embedding_dim = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e111cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.LSTM(lstm_units),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_sequence, labels, epochs=10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1045cc",
   "metadata": {},
   "source": [
    "#### Language Generation Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf145b71",
   "metadata": {},
   "source": [
    "1.\ttext = '''Natural language processing (NLP) is an interdisciplinary subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
    "\n",
    "# Create tokenizer with character-level tokenization\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessor(text,tokenizer,train=True):\n",
    "    # Tokenize the sentences at the character level\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(text)\n",
    "    # Create tokenized sequence by unlisting the tokens\n",
    "    tokenized_sequence = [char[0] for char in tokenized_sentences]\n",
    "    # Define length of a sequence\n",
    "    length = 10\n",
    "    if train:\n",
    "        dataset = []  \n",
    "\n",
    "        for i in range(length,len(tokenized_sequence)):\n",
    "            # Select the sequence for the desired length\n",
    "            seq = tokenized_sequence[i-length:i+1]\n",
    "            # Append the data set\n",
    "            dataset.append(seq)\n",
    "        dataset = np.array(dataset) \n",
    "\n",
    "        # Generate the sequence and the target\n",
    "        X,Y = dataset[:,:-1],dataset[:,-1]\n",
    "        # Get the Vocab size also\n",
    "        vocab_size = len(set(tokenized_sequence))\n",
    "        # Reshaping the target to convert to one hot encoded format\n",
    "        Y_reshape = Y.reshape(-1, 1)\n",
    "        # Converting target to one hot encoded shape\n",
    "        y = to_categorical(Y_reshape, num_classes=vocab_size+1)\n",
    "        return X,y,vocab_size\n",
    "    else:\n",
    "        return [tokenized_sequence[-length:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110dc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset.\n",
    "dataset,target,vocab_size = textProcessor(text,tokenizer)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size+1, embedding_dim, input_length=dataset.shape[1]))\n",
    "model.add(LSTM(64, return_sequences=True)),\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(vocab_size+1, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "model.fit(dataset,target, epochs=20, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text,tokenizer, max_len=100):\n",
    "    \"\"\"Generates text from an LSTM model.\"\"\"\n",
    "    generated_text = start_text\n",
    "    for _ in range(max_len):\n",
    "        # Get the next character prediction.\n",
    "        prediction = model.predict(textProcessor(generated_text,tokenizer,False))\n",
    "        # Select the most likely character.\n",
    "        next_char = np.argmax(prediction)\n",
    "        # Convert it back to character\n",
    "        char_converted = tokenizer.sequences_to_texts(np.array([next_char]).reshape(-1, 1))[0]\n",
    "        # Append the next character to the generated text.\n",
    "        generated_text += char_converted\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da72158",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_text(model, \"This is a sample text\",tokenizer, max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f529b9",
   "metadata": {},
   "source": [
    "#### Encoder Decoder Architecture\n",
    "\n",
    "The below is a sample code, to demonstrate the concepts. The full implementation can be found in the following link\n",
    "\n",
    "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample model for encoder decoder architecture\n",
    "model = Sequential()\n",
    "# Encoder \n",
    "\n",
    "model.add(Embedding(enc_vocab, n_units, input_length=enc_timesteps, mask_zero=True))\n",
    "model.add(LSTM(n_units))\n",
    "# Decoder \n",
    "\n",
    "model.add(RepeatVector(dec_timesteps))\n",
    "model.add(LSTM(n_units, return_sequences=True)) \n",
    "\n",
    "model.add(TimeDistributed(Dense(dec_vocab, activation='softmax'))) \n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b1e68",
   "metadata": {},
   "source": [
    "#### Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dimensions\n",
    "hidden_state = 16\n",
    "time_sequence = 4\n",
    "# Defining the input layers\n",
    "Query = Input(shape=(hidden_state,), name='Query')\n",
    "Key = Input(shape=(time_sequence,hidden_state), name='Key')\n",
    "\n",
    "# Expanding the dimension of Query\n",
    "Query_with_time_axis = tf.expand_dims(Query, 1,name=\"Query_expansion_layer\")\n",
    "\n",
    "# Dense Layers for each of the inputs \n",
    "\n",
    "W_Query = tf.keras.layers.Dense(hidden_state,name=\"Query_dense_layer\")\n",
    "W_Key = tf.keras.layers.Dense(hidden_state,name=\"Key_dense_layer\")\n",
    "V_consolidated = tf.keras.layers.Dense(1,name=\"Consolidated_dense_layer\")\n",
    "\n",
    "# Getting the Score \n",
    "\n",
    "scores = V_consolidated(tf.nn.tanh(W_Query(Query_with_time_axis) + W_Key(Key)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cb4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving the normalized scores using Softmax\n",
    "Attention_weight = tf.nn.softmax(scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59256b0a",
   "metadata": {},
   "source": [
    "#### RNN with attention Mechanism\n",
    "\n",
    "Please download the data set from the following link\n",
    "\n",
    "https://github.com/Rishav09/Neural-Machine-Translation-System/blob/master/english-german-both.pkl\n",
    "\n",
    "You can download the pickeled data and then store it in your local drive under a folder 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"data/english-german-both.pkl\"\n",
    "\n",
    "# Load a clean dataset\n",
    "clean_dataset = load(open(dataPath, 'rb'))\n",
    "clean_dataset\n",
    "\n",
    "n_sentences = 10000\n",
    "train_split = 0.9\n",
    "\n",
    "# Reduce dataset size\n",
    "dataset = clean_dataset[:n_sentences, :]\n",
    "\n",
    "# Random shuffle the dataset\n",
    "shuffle(dataset)\n",
    "\n",
    "# Split the dataset\n",
    "train = dataset[:int(n_sentences * train_split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647104d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Fit a tokenizer\n",
    "def create_tokenizer(dataset): \n",
    "    tokenizer = Tokenizer() \n",
    "    tokenizer.fit_on_texts(dataset)\n",
    "    return tokenizer\n",
    "\n",
    "def find_seq_length(dataset):\n",
    "    return max(len(seq.split()) for seq in dataset)\n",
    "\n",
    "def find_vocab_size(tokenizer, dataset):\n",
    "    tokenizer.fit_on_texts(dataset)\n",
    "    return len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "# Prepare tokenizer for the encoder input\n",
    "enc_tokenizer = create_tokenizer(train[:, 0])\n",
    "enc_seq_length = find_seq_length(train[:, 0])\n",
    "enc_vocab_size = find_vocab_size(enc_tokenizer, train[:, 0])\n",
    "\n",
    "\n",
    "# Encode and pad the input sequences\n",
    "trainX = enc_tokenizer.texts_to_sequences(train[:, 0])\n",
    "trainX\n",
    "\n",
    "trainX = pad_sequences(trainX, maxlen=enc_seq_length, padding='post')\n",
    "trainX\n",
    "\n",
    "trainX = convert_to_tensor(trainX, dtype=tf.float32)\n",
    "trainX\n",
    "\n",
    "# Prepare tokenizer for the decoder input\n",
    "dec_tokenizer = create_tokenizer(train[:, 1])\n",
    "dec_seq_length = find_seq_length(train[:, 1])\n",
    "dec_vocab_size = find_vocab_size(dec_tokenizer, train[:, 1])\n",
    "\n",
    "# Encode and pad the input sequences\n",
    "trainY = dec_tokenizer.texts_to_sequences(train[:, 1])\n",
    "trainY\n",
    "\n",
    "trainY = pad_sequences(trainY, maxlen=dec_seq_length, padding='post')\n",
    "trainY\n",
    "\n",
    "trainY = convert_to_tensor(trainY, dtype=tf.float32)\n",
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110bd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"batch_size\" : 64,\n",
    "          \"src_vocabSize\" : len(enc_tokenizer.word_index)+1,\n",
    "          \"tar_vocabSize\" : len(dec_tokenizer.word_index)+1,\n",
    "          \"steps_per_epoch\" :len(trainX)//64 ,\n",
    "          \"embedding_dim\" : 128,\n",
    "          \"hidden_units\" : 512  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size= int(config[\"batch_size\"])\n",
    "        self.encoder_hu=int(config[\"hidden_units\"])\n",
    "        self.embedding=tf.keras.layers.Embedding(int(config[\"src_vocabSize\"]), int(config[\"embedding_dim\"]), dtype=tf.float32)\n",
    "        self.lstm= tf.keras.layers.LSTM(self.encoder_hu, return_sequences=True,return_state=True, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, x, hidden_lyr):\n",
    "        # Create the embedding layer\n",
    "        x= self.embedding(x)\n",
    "        # Get the LSTM output\n",
    "        output, state_h, state_c = self.lstm(x,initial_state=hidden_lyr)\n",
    "        return output, [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,config):\n",
    "        super( BahdanauAttention, self).__init__()\n",
    "        self.W1= Dense(config[\"hidden_units\"], dtype=tf.float32)  # encoder output\n",
    "        self.W2= Dense(config[\"hidden_units\"], dtype=tf.float32)  # Decoder hidden\n",
    "        self.V= Dense(1, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        #calculate the Attention score\n",
    "        score= self.V(tf.nn.tanh(self.W1(values) + self.W2(tf.expand_dims(query, axis=1))))\n",
    "        # attention_weights \n",
    "        attention_weights= tf.nn.softmax(score, axis=1) # shape > (batch_size, src_vocab_size, 1)\n",
    "        # context_vector \n",
    "        context_vector= attention_weights * values # shape > (batch_size, src_vocab_size, HU)\n",
    "        # Reduce the context vector along the sequence axis\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1) # shape > (batch_size,HU)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922f96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super (Decoder,self).__init__()\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.decoder_hu = config[\"hidden_units\"]\n",
    "        self.embedding = Embedding(config[\"tar_vocabSize\"], config[\"embedding_dim\"], dtype=tf.float32)\n",
    "        self.lstm= LSTM(config[\"hidden_units\"], return_sequences= True,return_state=True,recurrent_initializer='glorot_uniform', dtype=tf.float32)\n",
    "        # Fully connected layer\n",
    "        self.fc= Dense(config[\"tar_vocabSize\"], dtype=tf.float32)\n",
    "        # attention\n",
    "        self.attention = BahdanauAttention(config)\n",
    "        \n",
    "    def call(self, x, query, values):\n",
    "        # Get the context vector \n",
    "        context_vector, attention_weights = self.attention(query,values)\n",
    "        # Pass the decoder sequence one time step at a time to get the embedding layer\n",
    "        x= self.embedding(x) # shape > (batch_size, 1, embedding dim)\n",
    "        #print(\"decoder input after embedding\",x.shape)\n",
    "        # Concatenate decoder input and context vector to get the hidden state and output\n",
    "        x= tf.concat([tf.expand_dims( context_vector, 1), x], axis=-1) # shape > (batch_size, 1, embedding dim + HU)\n",
    "        #print(\"decoder input after concatenation\",x.shape)\n",
    "        # Get the RNN output\n",
    "        output, state_h,state_c = self.lstm(x) # shape > (batch_size, 1,HU)\n",
    "        #print(\"Output shape raw\",output.shape)\n",
    "        # Reshape the output to remove the sequence\n",
    "        output= tf.reshape(output, (-1, output.shape[2])) # shape > (batch_size,HU)\n",
    "        #print(\"Output shape after reshape\",output.shape)\n",
    "        # Final decoder output\n",
    "        x= self.fc(output) # shape > (batch_size,target_vocab_size)\n",
    "        return x, [state_h,state_c], attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d18758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimizer and the loss function\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc580fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp, targ, encoder_hidden,config):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get the encoder outputs\n",
    "        encoder_output, [enc_state_h, enc_state_c]= encoder(inp, encoder_hidden)\n",
    "        # Initialise the decoder hidden state as the encoder hidden state\n",
    "        decoder_hidden = enc_state_h\n",
    "        # Initialise the decoder first input as 0. This can be altered to make it as start token \n",
    "        decoder_input = tf.expand_dims([0] * config[\"batch_size\"], 1)\n",
    "        # Iterate through each of the target sequence to get the predicted output and loss calculation\n",
    "        for t in range(targ.shape[1]):\n",
    "          # Generating the decoder output for the target sequence time step\n",
    "          predictions, [decoder_hidden,_], _ = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "          # Loss calculations for each sequence\n",
    "          loss += tf.keras.losses.sparse_categorical_crossentropy(targ[:, t], predictions)\n",
    "          # The next input will be the current target\n",
    "          decoder_input = tf.expand_dims(targ[:, t], 1)\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923d7e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data in memeory \n",
    "dataset=tf.data.Dataset.from_tensor_slices((trainX, trainY)).shuffle(config['batch_size'])\n",
    "# shuffles the data in the batch\n",
    "dataset = dataset.batch(config['batch_size'], drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e6b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "# Initialise encoder and decoder\n",
    "encoder = Encoder(config)\n",
    "decoder= Decoder(config)\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    # Initialise the encoder hidden shape\n",
    "    enc_hidden = [tf.zeros((config[\"batch_size\"], config[\"hidden_units\"])),tf.zeros((config[\"batch_size\"], config[\"hidden_units\"]))]\n",
    "    total_loss = 0\n",
    "    # Train the model in batches\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(config[\"steps_per_epoch\"])):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden,config)\n",
    "        total_loss += batch_loss\n",
    "        if batch % 100 == 0:\n",
    "          print('Epoch {} Batch {} Loss {}'.format(epoch + 1,\n",
    "                                                       batch,                                                   \n",
    "                                             batch_loss.numpy()))\n",
    "    print('Epoch {} Loss {}'.format(epoch + 1,\n",
    "                                      total_loss / config[\"steps_per_epoch\"]))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45529985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5224875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492c5c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab368b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
